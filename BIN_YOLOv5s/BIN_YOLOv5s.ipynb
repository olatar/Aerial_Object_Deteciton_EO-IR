{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BIN_v5small_NormBox.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyMyK4L39kdg6hG6v5hEVjbq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZTPFkSBeOVsO"},"source":["# YOLOv5s BIN\n","\n","This repository includes training of YOLOv5s object detection model with binary classes (object/no-object)."]},{"cell_type":"markdown","metadata":{"id":"cHK8ctRFOn8v"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"Ie5uLDH4uzAp"},"source":["# clone YOLOv5 and reset to a specific git checkpoint that has been verified working\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","!git reset --hard 68211f72c99915a15855f7b99bf5d93f5631330f"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbvMlHd_QwMG"},"source":["# install dependencies as necessary\n","!pip install -qr requirements.txt  # install dependencies (ignore errors)\n","import torch\n","\n","from IPython.display import Image, clear_output  # to display images\n","from utils.google_utils import gdrive_download  # to download models/datasets\n","\n","# clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SDIhrBF0sPaM"},"source":["# Download Correctly Formatted Custom Dataset \n","\n","This project use Roboflow.com to convert the data to the correct format."]},{"cell_type":"code","metadata":{"id":"Knxi2ncxWffW"},"source":["# Export code snippet and paste here\n","%cd /content\n","!curl -L \"https://app.roboflow.com/ds/KQz9znArDQ?key=1csCGmPnxi\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZ3DmmGQztJj"},"source":["# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n","%cat data.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RB1_KEiT8dyl"},"source":["# Change data from MC to BIN\n","\n","Run for test, train and valid"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beiLQ8-78-9D","executionInfo":{"status":"ok","timestamp":1607410452304,"user_tz":-60,"elapsed":762,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"b26f702e-2336-40ac-b49c-60cdffdc77dd"},"source":["%%writefile /content/data.yaml\n","train: ../train/images\n","val: ../valid/images\n","\n","nc: 1\n","names: ['Maritime Object']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/data.yaml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xZLm8KUN8iB5"},"source":["import os\n","\n","directory = '/content/test/labels/' # Run for test, train and valid\n","\n","for navn in os.listdir(directory):\n","    if navn.endswith(\".txt\"):\n","\n","        # Open the file as read\n","        filename = open(directory + navn, \"r+\")\n","        # Create an array to hold write data\n","        new_file = []\n","        # Loop the file line by line\n","        for line in filename:\n","            # Split A,B on , and use first position [0], aka A, then add to the new array\n","            line_splitted = line.split(\" \")\n","            # Add\n","            print(line_splitted)\n","\n","            label = line_splitted[0]\n","            x_cen = float(line_splitted[1])\n","            y_cen = float(line_splitted[2])\n","            width = float(line_splitted[3])\n","            height = float(line_splitted[4])\n","\n","            new_string = str(0) + ' ' + str(x_cen) + ' ' + str(y_cen) + ' ' + str(width) + ' ' + str(height)\n","            new_file.append(new_string)\n","\n","        # Open the file as Write, loop the new array and write with a newline\n","        with open(directory + navn, \"w+\") as f:\n","            for i, item in enumerate(new_file):\n","                if i == (len(new_file) - 1):\n","                    f.write(item)\n","                else:\n","                    f.write(item + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwJx-2NHsYxT"},"source":["# Define Model Configuration and Architecture\n","\n","Change config YAML file for the model to be specified for the custom dataset."]},{"cell_type":"code","metadata":{"id":"dOPn9wjOAwwK"},"source":["# define number of classes based on YAML\n","import yaml\n","with open(\"data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"H1HMN2Ms-GTO","executionInfo":{"status":"ok","timestamp":1607410098032,"user_tz":-60,"elapsed":525,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"9ba24467-d87d-42cc-b94a-04ddd7abc71d"},"source":["num_classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"t14hhyqdmw6O"},"source":["#customize iPython writefile so we can write variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDxebz13RdRA"},"source":["%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n","\n","# parameters\n","nc: {num_classes}  # number of classes\n","depth_multiple: 0.33  # model depth multiple\n","width_multiple: 0.50  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUOiNLtMP5aG"},"source":["# Train the YOLOv5 Detector\n","\n","Arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** set the path to our yaml file\n","- **cfg:** specify our model configuration\n","- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n","- **name:** result names\n","- **nosave:** only save the final checkpoint\n","- **cache:** cache images for faster training"]},{"cell_type":"code","metadata":{"id":"1NcFxRcFdJ_O"},"source":["# time its performance\n","%%time\n","%cd /content/yolov5/\n","!python train.py --img 640 --batch 16 --epochs 750 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights /content/gdrive/MyDrive/0Thesis/Hi-Res/BIN_v5small_NormBox/runs/train/yolov5s_results/weights/best.pt --name yolov5s_results  --cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJVs_4zEeVbF"},"source":["# Evaluate Custom YOLOv5 Detector Performance"]},{"cell_type":"code","metadata":{"id":"bOy5KI2ncnWd"},"source":["# Start tensorboard\n","# logs save in the folder \"runs\"\n","%load_ext tensorboard\n","#%tensorboard --logdir runs\n","\n","%tensorboard --logdir /content/gdrive/MyDrive/0Thesis/Hi-Res/BIN_v5small_NormBox/runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3qM6T0W53gh"},"source":["# Export Trained Weights for Future Inference\n","\n","Export the trained weights to Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuiSye-S-Wmg","executionInfo":{"status":"ok","timestamp":1607410132714,"user_tz":-60,"elapsed":26466,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"d6336afd-6c6e-4712-c936-a3dee27ff4af"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_wRoCSEwtziM"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JI6qEn5fZTBk","executionInfo":{"status":"ok","timestamp":1607416894791,"user_tz":-60,"elapsed":844,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"5a454088-990c-439d-fe2f-fe32724b966c"},"source":["%%writefile /content/data.yaml\n","train: ../train/images\n","test: ../test/images\n","\n","nc: 1\n","names: ['Maritime Object']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/data.yaml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQiVaf66Z5Of","executionInfo":{"status":"ok","timestamp":1607420729985,"user_tz":-60,"elapsed":19700,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"5b986a0d-196f-4540-b05f-106184a51e93"},"source":["%cd /content/yolov5/\n","!python test.py --weights /content/gdrive/MyDrive/0Thesis/Hi-Res/BIN_v5small_NormBox/best.pt --img-size 640 --conf-thres 0.001 --data /content/data.yaml --task 'test' --verbose"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/yolov5\n","Namespace(augment=False, batch_size=32, conf_thres=0.001, data='/content/data.yaml', device='', exist_ok=False, img_size=640, iou_thres=0.6, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=False, single_cls=False, task='test', verbose=True, weights=['/content/gdrive/MyDrive/0Thesis/Hi-Res/BIN_v5small_NormBox/best.pt'])\n","Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)\n","\n","Fusing layers... \n","Model Summary: 232 layers, 7246518 parameters, 0 gradients\n","Scanning '../test/labels.cache' for images and labels... 81 found, 0 missing, 27 empty, 0 corrupted: 100% 81/81 [00:00<00:00, 834738.63it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:05<00:00,  1.97s/it]\n","                 all          81          89       0.747       0.955       0.945        0.49\n","Speed: 3.2/2.1/5.3 ms inference/NMS/total per 640x640 image at batch-size 32\n","Results saved to runs/test/exp9\n"],"name":"stdout"}]}]}