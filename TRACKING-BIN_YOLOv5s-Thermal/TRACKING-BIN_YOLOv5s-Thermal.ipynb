{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRACKING-BIN-YOLOv5s-Thermal.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM2zD1C5knD/PW+n4mMWNK8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"t0VISqDAS44H"},"source":["# YOLOv5s BIN Tracking Thermal\n","\n","This repository includes training of Binary Thermal Tracking og YOLOv5s object detection model."]},{"cell_type":"markdown","metadata":{"id":"XwoRZly7TKOQ"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"wbvMlHd_QwMG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608156321982,"user_tz":-60,"elapsed":10679,"user":{"displayName":"Ola_test Test","photoUrl":"","userId":"06496175701324816115"}},"outputId":"9ff5ec07-68a6-4c89-ce17-d5361240045b"},"source":["!git clone https://github.com/ultralytics/yolov5  # clone repo\n","!pip install -qr yolov5/requirements.txt  # install dependencies (ignore errors)\n","%cd yolov5\n","\n","import torch\n","from IPython.display import Image, clear_output  # to display images\n","from utils.google_utils import gdrive_download  # to download models/datasets\n","\n","clear_output()\n","print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setup complete. Using torch 1.7.0+cu101 CPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SDIhrBF0sPaM"},"source":["# Download Correctly Formatted Custom Dataset \n","\n","This project use Roboflow.com to convert the data to the correct format."]},{"cell_type":"code","metadata":{"id":"Knxi2ncxWffW"},"source":["# Export code snippet and paste here\n","%cd /content\n","!curl -L \" \" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZ3DmmGQztJj"},"source":["# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n","%cat data.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LD6qvsgXQRj2"},"source":["# Change classes from multi-class to binary"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iujzB_q4QS1J","executionInfo":{"status":"ok","timestamp":1608156333038,"user_tz":-60,"elapsed":675,"user":{"displayName":"Ola_test Test","photoUrl":"","userId":"06496175701324816115"}},"outputId":"d397823d-d095-4fee-b94d-3d55a91e540e"},"source":["%%writefile /content/data.yaml\n","train: ../train/images\n","val: ../valid/images\n","\n","nc: 1\n","names: ['Maritime Object']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing /content/data.yaml\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2Jtdgb5-QThz"},"source":["import os\n","\n","directory = '/content/valid/labels/'\n","\n","for navn in os.listdir(directory):\n","    if navn.endswith(\".txt\"):\n","\n","        # Open the file as read\n","        filename = open(directory + navn, \"r+\")\n","        # Create an array to hold write data\n","        new_file = []\n","        # Loop the file line by line\n","        for line in filename:\n","            # Split A,B on , and use first position [0], aka A, then add to the new array\n","            line_splitted = line.split(\" \")\n","            # Add\n","            print(line_splitted)\n","\n","            label = line_splitted[0]\n","            x_cen = float(line_splitted[1])\n","            y_cen = float(line_splitted[2])\n","            width = float(line_splitted[3])\n","            height = float(line_splitted[4])\n","\n","            new_string = str(0) + ' ' + str(x_cen) + ' ' + str(y_cen) + ' ' + str(width) + ' ' + str(height)\n","            new_file.append(new_string)\n","\n","        # Open the file as Write, loop the new array and write with a newline\n","        with open(directory + navn, \"w+\") as f:\n","            for i, item in enumerate(new_file):\n","                if i == (len(new_file) - 1):\n","                    f.write(item)\n","                else:\n","                    f.write(item + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_Esc4BtSTrv"},"source":["import os\n","\n","directory = '/content/train/labels/'\n","\n","for navn in os.listdir(directory):\n","    if navn.endswith(\".txt\"):\n","\n","        # Open the file as read\n","        filename = open(directory + navn, \"r+\")\n","        # Create an array to hold write data\n","        new_file = []\n","        # Loop the file line by line\n","        for line in filename:\n","            # Split A,B on , and use first position [0], aka A, then add to the new array\n","            line_splitted = line.split(\" \")\n","            # Add\n","            print(line_splitted)\n","\n","            label = line_splitted[0]\n","            x_cen = float(line_splitted[1])\n","            y_cen = float(line_splitted[2])\n","            width = float(line_splitted[3])\n","            height = float(line_splitted[4])\n","\n","            new_string = str(0) + ' ' + str(x_cen) + ' ' + str(y_cen) + ' ' + str(width) + ' ' + str(height)\n","            new_file.append(new_string)\n","\n","        # Open the file as Write, loop the new array and write with a newline\n","        with open(directory + navn, \"w+\") as f:\n","            for i, item in enumerate(new_file):\n","                if i == (len(new_file) - 1):\n","                    f.write(item)\n","                else:\n","                    f.write(item + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KUERoJbSUX8"},"source":["import os\n","\n","directory = '/content/test/labels/'\n","\n","for navn in os.listdir(directory):\n","    if navn.endswith(\".txt\"):\n","\n","        # Open the file as read\n","        filename = open(directory + navn, \"r+\")\n","        # Create an array to hold write data\n","        new_file = []\n","        # Loop the file line by line\n","        for line in filename:\n","            # Split A,B on , and use first position [0], aka A, then add to the new array\n","            line_splitted = line.split(\" \")\n","            # Add\n","            print(line_splitted)\n","\n","            label = line_splitted[0]\n","            x_cen = float(line_splitted[1])\n","            y_cen = float(line_splitted[2])\n","            width = float(line_splitted[3])\n","            height = float(line_splitted[4])\n","\n","            new_string = str(0) + ' ' + str(x_cen) + ' ' + str(y_cen) + ' ' + str(width) + ' ' + str(height)\n","            new_file.append(new_string)\n","\n","        # Open the file as Write, loop the new array and write with a newline\n","        with open(directory + navn, \"w+\") as f:\n","            for i, item in enumerate(new_file):\n","                if i == (len(new_file) - 1):\n","                    f.write(item)\n","                else:\n","                    f.write(item + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwJx-2NHsYxT"},"source":["# Define Model Configuration and Architecture\n","\n","Change config YAML file for the model to be specified for the custom dataset."]},{"cell_type":"code","metadata":{"id":"dOPn9wjOAwwK"},"source":["# define number of classes based on YAML\n","import yaml\n","with open(\"data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t14hhyqdmw6O"},"source":["#customize iPython writefile so we can write variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDxebz13RdRA"},"source":["%%writetemplate /content/yolov5/models/custom_yolov5l.yaml\n","\n","# parameters\n","nc: 1  # number of classes\n","depth_multiple: 1.0  # model depth multiple\n","width_multiple: 1.0  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUOiNLtMP5aG"},"source":["# Train the YOLOv5 Detector\n","\n","Arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** set the path to our yaml file\n","- **cfg:** specify our model configuration\n","- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n","- **name:** result names\n","- **nosave:** only save the final checkpoint\n","- **cache:** cache images for faster training"]},{"cell_type":"code","metadata":{"id":"1NcFxRcFdJ_O"},"source":["# time its performance\n","%%time\n","%cd /content/yolov5/\n","!python train.py --img 640 --batch 16 --epochs 1500 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJVs_4zEeVbF"},"source":["# Evaluate Custom YOLOv5 Detector Performance"]},{"cell_type":"code","metadata":{"id":"bOy5KI2ncnWd"},"source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wI_kbbm_T1gn"},"source":["# Evaluate on test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_jMUg8QYzuh","executionInfo":{"status":"ok","timestamp":1607414969439,"user_tz":-60,"elapsed":18446,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"840def69-7269-4428-ba06-9ccec3e91416"},"source":["%cd /content/yolov5/\n","!python test.py --weights '' --img-size 720 --conf-thres 0.01 --data /content/data.yaml --task 'test' --verbose"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/yolov5\n","Namespace(augment=False, batch_size=32, conf_thres=0.01, data='/content/data.yaml', device='', exist_ok=False, img_size=640, iou_thres=0.6, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=False, single_cls=False, task='test', verbose=True, weights=['/content/drive/MyDrive/0Thesis/Hi-Res/TRACK/BIN_v5-640/best.pt'])\n","Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)\n","\n","Fusing layers... \n","Model Summary: 232 layers, 7246518 parameters, 0 gradients\n","Scanning '../test/labels.cache' for images and labels... 81 found, 0 missing, 27 empty, 0 corrupted: 100% 81/81 [00:00<00:00, 630312.85it/s]\n","               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:05<00:00,  1.78s/it]\n","                 all          81          89       0.747       0.955       0.948       0.513\n","Speed: 4.2/3.4/7.7 ms inference/NMS/total per 640x640 image at batch-size 32\n","Results saved to runs/test/exp3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HzoQs56Zyi7-"},"source":["# Install SORT Tracker functions"]},{"cell_type":"code","metadata":{"id":"WJgYL6k5_fIK"},"source":["%cd /content/\n","!git clone https://github.com/abewley/sort.git  # clone repo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BJ6cgZm_xkU"},"source":["%cd /content/\n","!pip install imgaug==0.2.5\n","!pip install -qr sort/requirements.txt  # install dependencies (ignore errors)\n","%cd sort"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHz7YhGO0hO1"},"source":["# BYTTER STRUKTUR, SÅ TILGANG PÅ SORT BLIR TILGJENGELIG I DETECT\n","%cp -r /content/sort/ /content/yolov5/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UxE5FyeCb5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608156396722,"user_tz":-60,"elapsed":11179,"user":{"displayName":"Ola_test Test","photoUrl":"","userId":"06496175701324816115"}},"outputId":"a1e4a1ff-4d37-4f50-a6c5-4334cbfb919a"},"source":["%%writefile /content/yolov5/sort/sort.py\n","\n","from __future__ import print_function\n","\n","import os\n","import numpy as np\n","import matplotlib\n","#matplotlib.use('TkAgg')\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from skimage import io\n","\n","import glob\n","import time\n","import argparse\n","from filterpy.kalman import KalmanFilter\n","\n","np.random.seed(0)\n","\n","\n","def linear_assignment(cost_matrix):\n","  try:\n","    import lap\n","    _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n","    return np.array([[y[i],i] for i in x if i >= 0]) #\n","  except ImportError:\n","    from scipy.optimize import linear_sum_assignment\n","    x, y = linear_sum_assignment(cost_matrix)\n","    return np.array(list(zip(x, y)))\n","\n","\n","def iou_batch(bb_test, bb_gt):\n","  \"\"\"\n","  From SORT: Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n","  \"\"\"\n","  bb_gt = np.expand_dims(bb_gt, 0)\n","  bb_test = np.expand_dims(bb_test, 1)\n","  \n","  xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n","  yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n","  xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n","  yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n","  w = np.maximum(0., xx2 - xx1)\n","  h = np.maximum(0., yy2 - yy1)\n","  wh = w * h\n","  o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])                                      \n","    + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)                                              \n","  return(o)  \n","\n","\n","def convert_bbox_to_z(bbox):\n","  \"\"\"\n","  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n","    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n","    the aspect ratio\n","  \"\"\"\n","  w = bbox[2] - bbox[0]\n","  h = bbox[3] - bbox[1]\n","  x = bbox[0] + w/2.\n","  y = bbox[1] + h/2.\n","  s = w * h    #scale is just area\n","  r = w / float(h)\n","  return np.array([x, y, s, r]).reshape((4, 1))\n","\n","\n","def convert_x_to_bbox(x,score=None):\n","  \"\"\"\n","  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n","    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n","  \"\"\"\n","  w = np.sqrt(x[2] * x[3])\n","  h = x[2] / w\n","  if(score==None):\n","    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n","  else:\n","    return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n","\n","\n","class KalmanBoxTracker(object):\n","  \"\"\"\n","  This class represents the internal state of individual tracked objects observed as bbox.\n","  \"\"\"\n","  count = 0\n","  def __init__(self,bbox):\n","    \"\"\"\n","    Initialises a tracker using initial bounding box.\n","    \"\"\"\n","    #define constant velocity model\n","    self.kf = KalmanFilter(dim_x=7, dim_z=4) \n","    self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n","    self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n","\n","    self.kf.R[2:,2:] *= 10.\n","    self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n","    self.kf.P *= 10.\n","    self.kf.Q[-1,-1] *= 0.01\n","    self.kf.Q[4:,4:] *= 0.01\n","\n","    self.kf.x[:4] = convert_bbox_to_z(bbox)\n","    self.time_since_update = 0\n","    self.id = KalmanBoxTracker.count\n","    KalmanBoxTracker.count += 1\n","    self.history = []\n","    self.hits = 0\n","    self.hit_streak = 0\n","    self.age = 0\n","\n","  def update(self,bbox):\n","    \"\"\"\n","    Updates the state vector with observed bbox.\n","    \"\"\"\n","    self.time_since_update = 0\n","    self.history = []\n","    self.hits += 1\n","    self.hit_streak += 1\n","    self.kf.update(convert_bbox_to_z(bbox))\n","\n","  def predict(self):\n","    \"\"\"\n","    Advances the state vector and returns the predicted bounding box estimate.\n","    \"\"\"\n","    if((self.kf.x[6]+self.kf.x[2])<=0):\n","      self.kf.x[6] *= 0.0\n","    self.kf.predict()\n","    self.age += 1\n","    if(self.time_since_update>0):\n","      self.hit_streak = 0\n","    self.time_since_update += 1\n","    self.history.append(convert_x_to_bbox(self.kf.x))\n","    return self.history[-1]\n","\n","  def get_state(self):\n","    \"\"\"\n","    Returns the current bounding box estimate.\n","    \"\"\"\n","    return convert_x_to_bbox(self.kf.x)\n","\n","\n","def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n","  \"\"\"\n","  Assigns detections to tracked object (both represented as bounding boxes)\n","  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n","  \"\"\"\n","  if(len(trackers)==0):\n","    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n","\n","  iou_matrix = iou_batch(detections, trackers)\n","\n","  if min(iou_matrix.shape) > 0:\n","    a = (iou_matrix > iou_threshold).astype(np.int32)\n","    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n","        matched_indices = np.stack(np.where(a), axis=1)\n","    else:\n","      matched_indices = linear_assignment(-iou_matrix)\n","  else:\n","    matched_indices = np.empty(shape=(0,2))\n","\n","  unmatched_detections = []\n","  for d, det in enumerate(detections):\n","    if(d not in matched_indices[:,0]):\n","      unmatched_detections.append(d)\n","  unmatched_trackers = []\n","  for t, trk in enumerate(trackers):\n","    if(t not in matched_indices[:,1]):\n","      unmatched_trackers.append(t)\n","\n","  #filter out matched with low IOU\n","  matches = []\n","  for m in matched_indices:\n","    if(iou_matrix[m[0], m[1]]<iou_threshold):\n","      unmatched_detections.append(m[0])\n","      unmatched_trackers.append(m[1])\n","    else:\n","      matches.append(m.reshape(1,2))\n","  if(len(matches)==0):\n","    matches = np.empty((0,2),dtype=int)\n","  else:\n","    matches = np.concatenate(matches,axis=0)\n","\n","  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n","\n","\n","class Sort(object):\n","  def __init__(self, max_age=1, min_hits=3, iou_threshold=0.3):\n","    \"\"\"\n","    Sets key parameters for SORT\n","    \"\"\"\n","    self.max_age = max_age\n","    self.min_hits = min_hits\n","    self.iou_threshold = iou_threshold\n","    self.trackers = []\n","    self.frame_count = 0\n","\n","  def update(self, dets=np.empty((0, 5))):\n","    \"\"\"\n","    Params:\n","      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n","    Requires: this method must be called once for each frame even with empty detections (use np.empty((0, 5)) for frames without detections).\n","    Returns the a similar array, where the last column is the object ID.\n","    NOTE: The number of objects returned may differ from the number of detections provided.\n","    \"\"\"\n","    self.frame_count += 1\n","    # get predicted locations from existing trackers.\n","    trks = np.zeros((len(self.trackers), 5))\n","    to_del = []\n","    ret = []\n","    for t, trk in enumerate(trks):\n","      pos = self.trackers[t].predict()[0]\n","      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n","      if np.any(np.isnan(pos)):\n","        to_del.append(t)\n","    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n","    for t in reversed(to_del):\n","      self.trackers.pop(t)\n","    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks, self.iou_threshold)\n","\n","    # update matched trackers with assigned detections\n","    for m in matched:\n","      self.trackers[m[1]].update(dets[m[0], :])\n","\n","    # create and initialise new trackers for unmatched detections\n","    for i in unmatched_dets:\n","        trk = KalmanBoxTracker(dets[i,:])\n","        self.trackers.append(trk)\n","    i = len(self.trackers)\n","    for trk in reversed(self.trackers):\n","        d = trk.get_state()[0]\n","        if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n","          ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n","        i -= 1\n","        # remove dead tracklet\n","        if(trk.time_since_update > self.max_age):\n","          self.trackers.pop(i)\n","    if(len(ret)>0):\n","      return np.concatenate(ret)\n","    return np.empty((0,5))\n","\n","\n","def parse_args():\n","    \"\"\"Parse input arguments.\"\"\"\n","    parser = argparse.ArgumentParser(description='SORT demo')\n","    parser.add_argument('--display', dest='display', help='Display online tracker output (slow) [False]',action='store_true')\n","    parser.add_argument(\"--seq_path\", help=\"Path to detections.\", type=str, default='data')\n","    parser.add_argument(\"--phase\", help=\"Subdirectory in seq_path.\", type=str, default='train')\n","    parser.add_argument(\"--max_age\", \n","                        help=\"Maximum number of frames to keep alive a track without associated detections.\", \n","                        type=int, default=1)\n","    parser.add_argument(\"--min_hits\", \n","                        help=\"Minimum number of associated detections before track is initialised.\", \n","                        type=int, default=3)\n","    parser.add_argument(\"--iou_threshold\", help=\"Minimum IOU for match.\", type=float, default=0.3)\n","    args = parser.parse_args()\n","    return args\n","\n","if __name__ == '__main__':\n","  '''\n","  # all train\n","  args = parse_args()\n","  display = args.display\n","  phase = args.phase\n","  total_time = 0.0\n","  total_frames = 0\n","  colours = np.random.rand(32, 3) #used only for display\n","  if(display):\n","    if not os.path.exists('mot_benchmark'):\n","      print('\\n\\tERROR: mot_benchmark link not found!\\n\\n    Create a symbolic link to the MOT benchmark\\n    (https://motchallenge.net/data/2D_MOT_2015/#download). E.g.:\\n\\n    $ ln -s /path/to/MOT2015_challenge/2DMOT2015 mot_benchmark\\n\\n')\n","      exit()\n","    plt.ion()\n","    fig = plt.figure()\n","    ax1 = fig.add_subplot(111, aspect='equal')\n","\n","  if not os.path.exists('output'):\n","    os.makedirs('output')\n","  pattern = os.path.join(args.seq_path, phase, '*', 'det', 'det.txt')\n","  for seq_dets_fn in glob.glob(pattern):\n","    mot_tracker = Sort(max_age=args.max_age, \n","                       min_hits=args.min_hits,\n","                       iou_threshold=args.iou_threshold) #create instance of the SORT tracker\n","    seq_dets = np.loadtxt(seq_dets_fn, delimiter=',')\n","    seq = seq_dets_fn[pattern.find('*'):].split('/')[0]\n","    \n","    with open('output/%s.txt'%(seq),'w') as out_file:\n","      print(\"Processing %s.\"%(seq))\n","      for frame in range(int(seq_dets[:,0].max())):\n","        frame += 1 #detection and frame numbers begin at 1\n","        dets = seq_dets[seq_dets[:, 0]==frame, 2:7]\n","        dets[:, 2:4] += dets[:, 0:2] #convert to [x1,y1,w,h] to [x1,y1,x2,y2]\n","        total_frames += 1\n","\n","        if(display):\n","          fn = 'mot_benchmark/%s/%s/img1/%06d.jpg'%(phase, seq, frame)\n","          im =io.imread(fn)\n","          ax1.imshow(im)\n","          plt.title(seq + ' Tracked Targets')\n","\n","        start_time = time.time()\n","        trackers = mot_tracker.update(dets)\n","        cycle_time = time.time() - start_time\n","        total_time += cycle_time\n","\n","        for d in trackers:\n","          print('%d,%d,%.2f,%.2f,%.2f,%.2f,1,-1,-1,-1'%(frame,d[4],d[0],d[1],d[2]-d[0],d[3]-d[1]),file=out_file)\n","          if(display):\n","            d = d.astype(np.int32)\n","            ax1.add_patch(patches.Rectangle((d[0],d[1]),d[2]-d[0],d[3]-d[1],fill=False,lw=3,ec=colours[d[4]%32,:]))\n","\n","        if(display):\n","          fig.canvas.flush_events()\n","          plt.draw()\n","          ax1.cla()\n","\n","  print(\"Total Tracking took: %.3f seconds for %d frames or %.1f FPS\" % (total_time, total_frames, total_frames / total_time))\n","\n","  if(display):\n","    print(\"Note: to get real runtime results run without the option: --display\")\n","  '''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/yolov5/sort/sort.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t0mH0pNlbvXQ"},"source":["# Adding the SORT Tracker to the detection model with binary classes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUyZmC02b1k5","executionInfo":{"status":"ok","timestamp":1608156415682,"user_tz":-60,"elapsed":581,"user":{"displayName":"Ola_test Test","photoUrl":"","userId":"06496175701324816115"}},"outputId":"f7da9299-0bcb-482d-a29d-ebca9bd6f4ad"},"source":["%%writefile /content/yolov5/detect.py\n","\n","import argparse\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n","    strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","from sort.sort import *\n","\n","def detect(save_img=False):\n","    source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n","\n","    # Directories\n","    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    # Initialize\n","    set_logging()\n","    device = select_device(opt.device)\n","    half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","    # Load model\n","    model = attempt_load(weights, map_location=device)  # load FP32 model\n","    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n","    if half:\n","        model.half()  # to FP16\n","\n","    # Set Dataloader\n","    vid_path, vid_writer = None, None\n","\n","    save_img = True\n","    dataset = LoadImages(source, img_size=imgsz)\n","\n","    # Get names and colors\n","    names = model.module.names if hasattr(model, 'module') else model.names\n","    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(10)]\n","\n","    # Run inference\n","    t0 = time.time()\n","    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","    \n","    # Ola tull\n","    i = 0\n","\n","    #################### SORT\n","    mot_tracker = Sort(max_age=10, min_hits=10, iou_threshold=0.1) #create instance of the SORT tracker\n","    #mot_tracker = Sort(max_age=1, min_hits=3, iou_threshold=0.3) #create instance of the SORT tracker\n","    ####################\n","\n","    for path, img, im0s, vid_cap in dataset: # Kaller automatisk __next__\n","        # im0s / im0 = image, BGR, letterbox => for film 1280x720 (input str)\n","        # img = RGB, continous array => for detection 640 (gitt str)\n","        \n","        print('\\n\\nFØR-------------------------')\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","            img = img.unsqueeze(0)\n","\n","        # Inference\n","        t1 = time_synchronized()\n","        pred = model(img, augment=opt.augment)[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n","        t2 = time_synchronized()\n","        \n","        # Process detections\n","        print('\\nlen pred')\n","        print(len(pred))\n","        print(pred)\n","        print('\\n')\n","        for i, det in enumerate(pred):  # detections per image # KJØRER UANSETT KUN 1 GANG PER IMG???? JA; sjekket\n","            print('hva er så iter av det?: i, det')\n","            print(i)\n","            print(det)\n","\n","            p, s, im0 = Path(path), '', im0s\n","\n","            save_path = str(save_dir / p.name)\n","            txt_path = str(save_dir / 'labels' / p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n","            s += '%gx%g ' % img.shape[2:]  # print string\n","            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","\n","            \n","            ####################################################################\n","            # det er xy xy\n","            if len(det): # IF ANY DETECTIONS IN CURRENT LINE IN FRAME\n","                \n","                print('Trackers 1 Blir Kjørt')\n","\n","                # Rescale boxes from img_size to im0 size\n","                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","                \n","\n","                print(\"1240, 720 x,y,x,y conf, object_num (1-5)\")\n","                print(det[:, :4])\n","                # # . P(class)=P(class|obj)P(obj). Yolo3 har verdi for begge i hver BB (se yolov3 notion)\n","\n","\n","                #################### SORT ########################################\n","\n","                dets = det.cpu().detach().numpy()[:,:5]\n","                #[[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...] Her 1280, 720\n","\n","                # SCALE TO BIGGER BOUNDING BOX (beter IOU)\n","                dets[:,:2] -= 20\n","                dets[:,2:4] += 20\n","\n","                for row in dets:\n","                    # x1 y1 our of frame\n","                    if row[0] <= 0:\n","                        row[0] = 1\n","                    if row[1] <= 0:\n","                        row[1] = 1\n","\n","                    # x2 y2 our of frame\n","                    if row[2] >= im0.shape[1]:\n","                        row[2] = im0.shape[1] #linewidth buffer?\n","                    if row[3] >= im0.shape[0]:\n","                        row[3] = (im0.shape[0]-1) #linewidth buffer?\n","\n","                trackers = mot_tracker.update(dets)\n","\n","\n","                ### ATTACH LABEL AND CONF BACK TO FIGURE\n","                labels = det.cpu().detach().numpy()[:,5]\n","                labels = labels[::-1] # labels reversed after tracker apparently\n","                print(labels)\n","\n","                confidense = det.cpu().detach().numpy()[:,4]\n","                confidense = confidense[::-1]\n","                print(confidense)\n","\n","                # Skriver antall hver klassse detektert i bilde\n","                #for c in det[:, -1].unique():\n","                for c in det[:, -1].unique(): #TRACKING\n","                    n = (det[:, -1] == c).sum()  # detections per class\n","                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                # Write results\n","                for i, obj in enumerate(trackers):\n"," \n","\n","                #for *xyxy, conf, cls in reversed(det):\n","                #\n","                #    # DROPPER DETTE FOR TRACKING\n","                #    if save_img or view_img:  # BOUNDING BOX\n","                #        label = '%s %.2f' % (names[int(cls)], conf)\n","                #        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n","\n","                    if save_img or view_img:  # BOUNDING BOX\n","                        label = '%s %.2f %s' % (int(obj[4]), confidense[i], names[int(labels[i])])\n","                        plot_one_box(np.array(obj[:4]), im0, label=label, color=colors[int(obj[4]%10)], line_thickness=3)\n","\n","            else: # If no detections\n","                print('Trackers 2 Blir Kjørt')\n","                trackers = mot_tracker.update(np.empty((0, 5)))\n","\n","            ####################################################################\n","            \n","            # Print time (inference + NMS)\n","            print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","            '''\n","            # Stream results\n","            if view_img: # DEFAULT NEI\n","                cv2.imshow(p, im0)\n","                if cv2.waitKey(1) == ord('q'):  # q to quit\n","                    raise StopIteration\n","            '''\n","\n","            # Save results (image with detections)\n","            if save_img: # ALLTID TRUE, SÅ LENGE IKKE WEBCAM\n","                if dataset.mode == 'images': # NEI ER 'video'\n","                    cv2.imwrite(save_path, im0)\n","                else:\n","                    if vid_path != save_path:  # IF NEW VIDEO (ER KUN 1 GANG)\n","                        vid_path = save_path\n","                        if isinstance(vid_writer, cv2.VideoWriter):\n","                            vid_writer.release()  # release previous video writer\n","\n","                        fourcc = 'mp4v'  # output video codec\n","                        fps = vid_cap.get(cv2.CAP_PROP_FPS) # vid_cap = CAPTURER VideoCapture(der video source)\n","                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n","                    vid_writer.write(im0) # KUN DENNE BLIR UTFØRT ------ SKRIVER TIL FIL\n","\n","    if save_txt or save_img:\n","        print('Results saved to %s' % save_dir)\n","\n","    print('Done. (%.3fs)' % (time.time() - t0))\n","\n","\n","\n","def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n","    # Plots one bounding box on image img\n","    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","    color = color or [random.randint(0, 255) for _ in range(3)]\n","    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n","    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n","    if label:\n","        tf = max(tl - 1, 1)  # font thickness\n","        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n","        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n","        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n","    parser.add_argument('--source', type=str, default='data/images', help='source')  # file/folder, 0 for webcam\n","    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n","    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n","    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n","    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    parser.add_argument('--view-img', action='store_true', help='display results')\n","    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n","    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n","    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n","    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n","    parser.add_argument('--augment', action='store_true', help='augmented inference')\n","    parser.add_argument('--update', action='store_true', help='update all models')\n","    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n","    parser.add_argument('--name', default='exp', help='save results to project/name')\n","    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n","    opt = parser.parse_args()\n","    print(opt)\n","\n","    with torch.no_grad():\n","        if opt.update:  # update all models (to fix SourceChangeWarning)\n","            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n","                detect()\n","                strip_optimizer(opt.weights)\n","        else:\n","            detect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/yolov5/detect.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N3qM6T0W53gh"},"source":["# Evaluate model with tracker on video"]},{"cell_type":"code","metadata":{"id":"9nmZZnWOgJ2S"},"source":["# --conf 0.25 standard\n","%cd /content/yolov5/\n","!python detect.py --weights /content/drive/MyDrive/0thesis/Synch/Best/weights/best.pt --img 720 --conf 0.25 --source /content/drive/MyDrive/0thesis/Synch/Sped-Up_IR1.mp4 --name /content/drive/MyDrive/0thesis/Synch/4A_EO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dlrtKQuHUcNk"},"source":["# ---------------------"]},{"cell_type":"markdown","metadata":{"id":"YZJ6X4Fe_KgL"},"source":["# Adding the SORT Tracker to the detection model with multiple classes"]},{"cell_type":"code","metadata":{"id":"yCwcmYnx9h2S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607415433663,"user_tz":-60,"elapsed":1039,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"280617bc-a9de-4ffa-912a-e6c991fc2710"},"source":["%%writefile /content/yolov5/detect.py\n","\n","import argparse\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n","    strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","from sort.sort import *\n","\n","def detect(save_img=False):\n","    source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n","\n","    # Directories\n","    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    # Initialize\n","    set_logging()\n","    device = select_device(opt.device)\n","    half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","    # Load model\n","    model = attempt_load(weights, map_location=device)  # load FP32 model\n","    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n","    if half:\n","        model.half()  # to FP16\n","\n","    # Set Dataloader\n","    vid_path, vid_writer = None, None\n","\n","    save_img = True\n","    dataset = LoadImages(source, img_size=imgsz)\n","\n","    # Get names and colors\n","    names = model.module.names if hasattr(model, 'module') else model.names\n","    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","    # Run inference\n","    t0 = time.time()\n","    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","    \n","    # Ola tull\n","    i = 0\n","\n","    #################### SORT\n","    mot_tracker = Sort(max_age=10, min_hits=10, iou_threshold=0.1) #create instance of the SORT tracker\n","    #mot_tracker = Sort(max_age=1, min_hits=3, iou_threshold=0.3) #create instance of the SORT tracker\n","    ####################\n","\n","    for path, img, im0s, vid_cap in dataset: # Kaller automatisk __next__\n","        # im0s / im0 = image, BGR, letterbox => for film 1280x720 (input str)\n","        # img = RGB, continous array => for detection 640 (gitt str)\n","        \n","        print('\\n\\nFØR-------------------------')\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","            img = img.unsqueeze(0)\n","\n","        # Inference\n","        t1 = time_synchronized()\n","        pred = model(img, augment=opt.augment)[0]\n","\n","        # Apply NMS\n","        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n","        t2 = time_synchronized()\n","        \n","        # Process detections\n","        print('\\nlen pred')\n","        print(len(pred))\n","        print(pred)\n","        print('\\n')\n","        for i, det in enumerate(pred):  # detections per image # KJØRER UANSETT KUN 1 GANG PER IMG???? JA; sjekket\n","            print('hva er så iter av det?: i, det')\n","            print(i)\n","            print(det)\n","\n","            p, s, im0 = Path(path), '', im0s\n","\n","            save_path = str(save_dir / p.name)\n","            txt_path = str(save_dir / 'labels' / p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n","            s += '%gx%g ' % img.shape[2:]  # print string\n","            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","\n","            \n","            ####################################################################\n","            # det er xy xy\n","            if len(det): # IF ANY DETECTIONS IN CURRENT FRAME (Always > 1, so I dont understand\n","                \n","                print('Trackers 1 Blir Kjørt')\n","\n","                # Rescale boxes from img_size to im0 size\n","                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","                \n","\n","                print(\"1240, 720 x,y,x,y conf, object_num (1-5)\")\n","                print(det[:, :4])\n","                # # . P(class)=P(class|obj)P(obj). Yolo3 har verdi for begge i hver BB (se yolov3 notion)\n","\n","\n","                #################### SORT ########################################\n","\n","                dets = det.cpu().detach().numpy()[:,:5]\n","                #[[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...] Her 1280, 720\n","\n","                # SCALE TO BIGGER BOUNDING BOX (beter IOU)\n","                dets[:,:2] -= 20\n","                dets[:,2:4] += 20\n","\n","                for row in dets:\n","                    # x1 y1 our of frame\n","                    if row[0] <= 0:\n","                        row[0] = 1\n","                    if row[1] <= 0:\n","                        row[1] = 1\n","\n","                    # x2 y2 our of frame\n","                    if row[2] >= im0.shape[1]:\n","                        row[2] = im0.shape[1] #linewidth buffer?\n","                    if row[3] >= im0.shape[0]:\n","                        row[3] = (im0.shape[0]-1) #linewidth buffer?\n","\n","                trackers = mot_tracker.update(dets)\n","\n","\n","                ### ATTACH LABEL AND CONF BACK TO FIGURE\n","                labels = det.cpu().detach().numpy()[:,5]\n","                labels = labels[::-1] # labels reversed after tracker apparently\n","                print(labels)\n","\n","                confidense = det.cpu().detach().numpy()[:,4]\n","                confidense = confidense[::-1]\n","                print(confidense)\n","\n","                # Skriver antall hver klassse detektert i bilde\n","                #for c in det[:, -1].unique():\n","                for c in det[:, -1].unique(): #TRACKING\n","                    n = (det[:, -1] == c).sum()  # detections per class\n","                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                # Write results\n","                for i, obj in enumerate(trackers):\n"," \n","\n","                #for *xyxy, conf, cls in reversed(det):\n","                #\n","                #    # DROPPER DETTE FOR TRACKING\n","                #    if save_img or view_img:  # BOUNDING BOX\n","                #        label = '%s %.2f' % (names[int(cls)], conf)\n","                #        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n","\n","                    if save_img or view_img:  # BOUNDING BOX\n","                        label = '%s %.2f %s' % (int(obj[4]), confidense[i], names[int(labels[i])])\n","                        plot_one_box(np.array(obj[:4]), im0, label=label, color=colors[int(obj[4])%4], line_thickness=3)\n","\n","            else: # If no detections\n","                print('Trackers 2 Blir Kjørt')\n","                trackers = mot_tracker.update(np.empty((0, 5)))\n","\n","            ####################################################################\n","            \n","            # Print time (inference + NMS)\n","            print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","            '''\n","            # Stream results\n","            if view_img: # DEFAULT NEI\n","                cv2.imshow(p, im0)\n","                if cv2.waitKey(1) == ord('q'):  # q to quit\n","                    raise StopIteration\n","            '''\n","\n","            # Save results (image with detections)\n","            if save_img: # ALLTID TRUE, SÅ LENGE IKKE WEBCAM\n","                if dataset.mode == 'images': # NEI ER 'video'\n","                    cv2.imwrite(save_path, im0)\n","                else:\n","                    if vid_path != save_path:  # IF NEW VIDEO (ER KUN 1 GANG)\n","                        vid_path = save_path\n","                        if isinstance(vid_writer, cv2.VideoWriter):\n","                            vid_writer.release()  # release previous video writer\n","\n","                        fourcc = 'mp4v'  # output video codec\n","                        fps = vid_cap.get(cv2.CAP_PROP_FPS) # vid_cap = CAPTURER VideoCapture(der video source)\n","                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n","                    vid_writer.write(im0) # KUN DENNE BLIR UTFØRT ------ SKRIVER TIL FIL\n","\n","    if save_txt or save_img:\n","        print('Results saved to %s' % save_dir)\n","\n","    print('Done. (%.3fs)' % (time.time() - t0))\n","\n","\n","\n","def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n","    # Plots one bounding box on image img\n","    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","    color = color or [random.randint(0, 255) for _ in range(3)]\n","    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n","    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n","    if label:\n","        tf = max(tl - 1, 1)  # font thickness\n","        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n","        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n","        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n","    parser.add_argument('--source', type=str, default='data/images', help='source')  # file/folder, 0 for webcam\n","    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n","    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n","    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n","    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    parser.add_argument('--view-img', action='store_true', help='display results')\n","    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n","    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n","    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n","    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n","    parser.add_argument('--augment', action='store_true', help='augmented inference')\n","    parser.add_argument('--update', action='store_true', help='update all models')\n","    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n","    parser.add_argument('--name', default='exp', help='save results to project/name')\n","    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n","    opt = parser.parse_args()\n","    print(opt)\n","\n","    with torch.no_grad():\n","        if opt.update:  # update all models (to fix SourceChangeWarning)\n","            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n","                detect()\n","                strip_optimizer(opt.weights)\n","        else:\n","            detect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/yolov5/detect.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I0CBLKMQGWZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607415434095,"user_tz":-60,"elapsed":1463,"user":{"displayName":"Ola Tranum","photoUrl":"https://lh5.googleusercontent.com/-IRdqnmPZ8x0/AAAAAAAAAAI/AAAAAAAAADc/AZKV6LdqcWM/s64/photo.jpg","userId":"16412047261917522426"}},"outputId":"280bfb0c-4043-45af-ac04-c6a7923e9854"},"source":["print('Hey')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hey\n"],"name":"stdout"}]}]}
